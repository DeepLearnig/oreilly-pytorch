{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Embeddings and Recurrent Neural Networks with PyTorch\n",
    "- [@AlfredoCanziani](https://twitter.com/alfredocanziani)\n",
    "- [@GokuMohandas](https://twitter.com/GokuMohandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "from pycrayon import (\n",
    "    CrayonClient,\n",
    ")\n",
    "\n",
    "from IPython import (\n",
    "    display,\n",
    ")\n",
    "\n",
    "from IPython.display import (\n",
    "    Image,\n",
    "    clear_output,\n",
    ")\n",
    "\n",
    "# get matplotlib configuration\n",
    "%matplotlib inline\n",
    "%run plot_conf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import (\n",
    "    Variable,\n",
    ")\n",
    "\n",
    "from torch.nn import (\n",
    "    init,\n",
    ")\n",
    "\n",
    "from torchvision import (\n",
    "    datasets, \n",
    "    transforms,\n",
    "    models,\n",
    "    utils,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter, we learned how to represent text using one-hot encoded vectors. Wether it be at the word-level or char-level, we represent each token with a vector. Where the length of the vector was the number of unique tokens and was a zero vector except at once location (unique to the token). THis allowed us to represent our input text and use with our differentiable network's weights. This one-hot encoding method of representing text can quickly grow out of control as the size of our vocabulary increases. Not only do out input vector grow but the weights in our model will also grow accordingly.\n",
    "\n",
    "$$ \\text{I: } [1, 0, 0] \\\\ \\text{am: } [0, 1, 0] \\\\ \\text{good: } [0, 0, 1] $$\n",
    "\n",
    "We also face another issue with representing the data as one-hot encoded vector. When we read text, the different tokens (words) have meaning to us and we can draw relationship between each of them. When we chose to represent the tokens with a one-hot encoded represetnation, all semantic and functional relationship is lost. An arbitrary index is used to represent each of our tokens and there is no way to draw meaning from the vectors. \n",
    "\n",
    "So, to solve both the issue of high dimensional representations and a meaningless representation, we can use embeddings to represent our text. Embeddings offer a low level representation that can capture the relationship between our tokens as well. \n",
    "\n",
    "Typically, embedding sizes range between 50-300 for each token which is largely empirically chosen. Let's start by looking at what a typical embedding matrix looks like. Suppose we have N tokens and we choose the emebdding size to be 300. Then our embedding matrix (weights) will be of size [N X 300], where N is the number of unique tokens. As with one-hot encoding, each token still has its own vector represtation but this time the number of columns is limited to 300 regardless of the number of unique tokens. Our embeddings can be randomly initialized but we can also initalize them. It is common to see random uniform initializations with range [âˆ’sqrt(3), sqrt(3)] or be uniform in the unit cube ([-1.0, 1.0]). \n",
    "\n",
    "$$ \\text{embeddings: } [N X E] \\\\ \\text{N: # of unique tokens} \\\\ \\text{E: embedding size} $$\n",
    "\n",
    "Now how do we represent our inputs using embeddings? For each token in our input, we will take the appropriate vector representation. For example, suppose we have word level emebddings (size 300) and we want to represent a five word long sentence with embeddings. We gather the embedding for each token to receive a input sentence representation that has dimensions [5 X 300]. Now we can go ahead and do additional evaluations on this vector for our task.\n",
    "\n",
    "You may be wondering how random initializations can capture the relationship between our tokens and you are justified in thinking that because they do not, yet. Once we initalize our embeddings, we have several methods of manipulating the embeddings so that they are able to capture meaningful relationships. We will explore these various methods in detail in section 4.2, but the general premise is that we will use a feed forward network to build these meaningful embeddings in an unsupervised learning fashion. Once the embeddings are set, then we can use them for our specific task while keeping the emebddings frozen or still allow them to change. We can also choose to keep them randomly initialized and alter the embeddings while we train for our specific task. The later option usually leads to overfitting and does not capture true semantic relationships between the tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PyTorch, it is very easy to create embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Model with random embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Initialize weights.\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=input_size,\n",
    "            embedding_dim=hidden_size,\n",
    "        )\n",
    "        \n",
    "        # Properly initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Properly initialize weights.\n",
    "        \"\"\"\n",
    "        self.embedding.weight.data.uniform_(\n",
    "            -np.sqrt(3), np.sqrt(3))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        # Embed the inputs\n",
    "        embedded_inputs = self.embedding(inputs)\n",
    "        \n",
    "        # Rest of forward pass\n",
    "        ...\n",
    "        \n",
    "        return embedded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4\n",
      " 2\n",
      " 4\n",
      " 1\n",
      "[torch.LongTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input: List of token ids for a sentence\n",
    "X = Variable(torch.LongTensor([4, 2, 4, 1]))\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.6745 -1.0047 -1.5146  ...  -0.1317 -0.0313 -1.6230\n",
      "-0.4109  0.0476  0.1311  ...  -1.2627 -0.9952  0.9369\n",
      "-1.6745 -1.0047 -1.5146  ...  -0.1317 -0.0313 -1.6230\n",
      " 0.0145  0.9278 -1.2476  ...   1.1719  0.6486  1.1977\n",
      "[torch.FloatTensor of size 4x300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feed into the model to embed\n",
    "model = Model(input_size=5, hidden_size=300)\n",
    "embedded_inputs = model(X)\n",
    "print (embedded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore initiailizing our emebddings with pre-learned vectors in the next section when we cover them in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How do we ensure that our embeddings are meaningful? As we discussed earlier, one option is to initialize them and then to have them be \"unfrozen\". What this means is that we allow the embeddings to change and are treated like regular weights in terms of parameter updates. This is one particular approach is often not used due to risk of overfitting. Since the embeddings will be changed in the direction that minimizes the objective function, they may not be generalizable embeddings for test time. They may not reflect the semantic relationships between the words and are tailored for the training task. An alternative to this approach is to learn the embeddings such that the similarity in the embedding space reflects the semantic simialrity between the words. \n",
    "\n",
    "One popular approach is the Word2Vec model proposed by Mikolov et. al. (https://arxiv.org/abs/1310.4546). The premise of the model was to embed words in $\\mathbb{R}^d$ so that their inner product is maximized when words co-occur in similar contexts. A very large text corpora is used to do this in order to capture semantic meaning in the embeddings.\n",
    "\n",
    "#### Skip-gram model\n",
    "\n",
    "The model approach behind Word2vec is the skip-gram model. This model takes in a word and tries to predict the context words. The context words are considered to be the words to the right and left of the target word. The number of words we want to predict for either size depends on the window size. Let's look at an example to make this more concrete.\n",
    "\n",
    "$$ \\text{the sun was shining brightly today} $$\n",
    "\n",
    "We want to create pairs of data points ($(context, target)$) from this input sentence which we will use in our skip-gram model to learn the proper embeddings. Let's assume that the window size is 1.\n",
    "\n",
    "<img src=\"figures/ch04_skip_gram_data.jpg\"></img>\n",
    "\n",
    "With skip-gram, we want to predict each context word from the target word, so our dataset now looks like this:\n",
    "\n",
    "$$\\text{(sun, the), (sun, was), (was, sun), (was, shining), ... }$$\n",
    "\n",
    "We feed in the target word and wan to receive the context word for the output. For our model, this means we want to input the emebedding for the target word and receive a result that decodes to the context word. You'll notice that particular target word maps to different context words. This is what we want because we are not trying to learn a one-to-one relationship but how each word relates to all the other words in our dataset. The target word's embedding gets updated with every run and as the performance of the model increases, our embeddings are able to capture more the semantic relationships. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skip_gram(sentences, window_size):\n",
    "    \"\"\"\n",
    "    Create data based on\n",
    "    skip-gram approach aka\n",
    "    (predict context word\n",
    "    from target word).\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "\n",
    "        # For each index\n",
    "        for i, index in enumerate(sentence):\n",
    "\n",
    "            # Collect valid context indexes\n",
    "            contexts = []\n",
    "            for window in range(window_size):\n",
    "\n",
    "                # left side\n",
    "                if i-(window+1) >= 0:\n",
    "                    contexts.append(sentence[i-(window+1)])\n",
    "                # right side\n",
    "                if i+(window+1) < len(sentence):\n",
    "                    contexts.append(sentence[i+(window+1)])\n",
    "\n",
    "            # Add to data\n",
    "            for context in contexts:\n",
    "                data.append((index, context))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `skip_gram` function allows us to create training based on the ksip-gram approach. Using a window_size, we gather context words from the left and right of our target word and create pairs where we try to predict the context word from the target.\n",
    "\n",
    "<img src=\"figures/ch04_skipgram.jpg\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, D_in, H):\n",
    "        \"\"\"\n",
    "        Initialize weights.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "         # Embedding\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=D_in,\n",
    "            embedding_dim=H,\n",
    "            )\n",
    "        self.fc2 = nn.Linear(H, D_in)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights.\n",
    "        \"\"\"\n",
    "        self.embedding.weight.data.uniform_(-np.sqrt(3), np.sqrt(3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        z = self.embedding(x)\n",
    "        z = self.fc2(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training model is a simple `MLP` without any non-linearities. The objective is to use the context word's embedding to predict the target word. By repeatedly doing this using our training data, we will learn the semantic relationships between the words.\n",
    "\n",
    "The trained some sample text (Wizard of Oz) using our simple skip_gram model and we are already able to map a few basic semantic relations. Below are a few similar words to the word `witch`. Similarity is based on the cosine distance between the embedding for `witch` and all the other words in our corpus.\n",
    "\n",
    "```bash\n",
    "Close to witch : ['wicked', 'evil', 'messenger', 'mouth', 'pot', 'heels', 'cooking', 'believe', 'pigs', 'mole']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pretrained Embeddings in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "bash fetch_embeddings.sh (in lesson3 directory)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pytorch_code.glove.main import (\n",
    "    get_embedding,\n",
    "    load_embeddings,\n",
    "    get_closest,\n",
    "    pretty_print,\n",
    "    get_analogy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "embedding_dir = os.path.join(\"embeddings/glove\")\n",
    "embedding_dim = 100 # 50, 100, 200, 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-23 10:26:02,631 ==> Loading Embeddings ...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400000/400000 [00:37<00:00, 10570.10it/s]\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"==> Loading Embeddings ...\")\n",
    "word_to_idx, embeddings = load_embeddings(\n",
    "    embedding_dir=embedding_dir,\n",
    "    embedding_dim=embedding_dim,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Nearest neighbors\n",
    "\n",
    "We will now use our pretrained embeddings to extract some of the relationships that were captured. We can use a simple distance metric to find out the nearest neighbors to a particular word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word: gucci\n",
      "[3.87] - burberry\n",
      "[4.00] - prada\n",
      "[4.01] - chanel\n",
      "[4.02] - dior\n",
      "[4.31] - fendi\n"
     ]
    }
   ],
   "source": [
    "word = input('Enter a word: ')\n",
    "word_embedding = get_embedding(word, word_to_idx, embeddings)\n",
    "pretty_print(get_closest(word_embedding, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector arithmetic\n",
    "We can also do some vector arithmetic to see analogies as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[man : king :: woman : ?]\n",
      "[4.08] - queen\n",
      "[4.64] - monarch\n",
      "[4.91] - throne\n",
      "[4.92] - elizabeth\n",
      "[4.98] - prince\n"
     ]
    }
   ],
   "source": [
    "word1, word2, word3 = \"man\", \"king\", \"woman\"\n",
    "pretty_print(get_analogy(word1, word2, word3, word_to_idx, embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by taking the embedding for `man` and subtract it from the embedding for `king`. This resulting vector is then added with the embedding for `woman`. We take this final representation and find the closest words to it. But why does this work? Since the embeddings are able to capture the relationship between the words, we can think of the vector we receive from `king`-`man` as the representation for trasnforming `man` to `king`. When we add this vector to `woman`, we apply the same transformation and we receive the expected 'queen' as one of the closest words. \n",
    "\n",
    "Now suppose we subtracted the embedding for `king` from `man`, which means the representation now holds the information to transform a king to a man. You can think of `man` as a younger version of `king` and also a more general term. When we add this difference to `woman`, we receive words like `girl` and `person`. Both of which are in accordance with our younger and more general expectations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[king : man :: woman : ?]\n",
      "[7.17] - girl\n",
      "[7.64] - person\n",
      "[7.72] - victim\n",
      "[7.78] - boy\n",
      "[7.80] - teenager\n"
     ]
    }
   ],
   "source": [
    "word1, word2, word3 = \"king\", \"man\", \"woman\"\n",
    "pretty_print(get_analogy(word1, word2, word3, word_to_idx, embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try more examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter three words to fill ____ : ____ :: _____ banana yellow grapes\n",
      "[banana : yellow :: grapes : ?]\n",
      "[6.64] - purple\n",
      "[6.77] - red\n",
      "[6.99] - colors\n",
      "[7.01] - bright\n",
      "[7.15] - daffodils\n"
     ]
    }
   ],
   "source": [
    "word1, word2, word3 = input(\"Enter three words to fill ____ : ____ :: _____ \").split()\n",
    "pretty_print(get_analogy(word1, word2, word3, word_to_idx, embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize embeddings\n",
    "\n",
    "We can also visualize the embeddings to see clusters, etc. We will use [t-Distributed Stochastic Neighbor Embedding (t-SNE)](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf) to visusalize our high dimensional embeddings on a 2D plane. The method allows us to take high-dimensional inputs and gives us a lower-dimensional representation. In our case, we are converting our high-dimensional embeddings to two dimensions so we can visualize them on a 2D plane. t-SNE applies non-linear transformation to regions of the initial input while maintaining the complex relationships between our points as much as possible. The algorithm involves several complicated hyperparameters such as perplexity, etc. but we do not need to address paritculars for our visualization purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6YAAAHOCAYAAACLuhT1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmAIAAFAogykAAACF+v8BM8/bcw9MdnQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1336d65c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve embeddings for words\n",
    "words = [\"man\", \"king\", \"woman\", \"queen\"]\n",
    "my_embeddings = np.array([\n",
    "    get_embedding(word, word_to_idx, embeddings).numpy() \\\n",
    "    for word in words])\n",
    "\n",
    "# Use TSNE model to reduce dimensionality\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "points = model.fit_transform(my_embeddings) \n",
    "\n",
    "# Visualize\n",
    "for i, word in enumerate(words):\n",
    "    x, y = points[i, 0]*1e4, points[i, 1]*1e4\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(25, 5),\n",
    "        textcoords='offset points', ha='right', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pytorch.org/docs/master/nn.html#recurrent-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, num_unique_words, embedding_dim,\n",
    "        num_hidden_units, num_layers, dropout_p, embeddings=None):\n",
    "        \"\"\"\n",
    "        Initialize weights.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_hidden_units = num_hidden_units\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embeddings\n",
    "        self.embeddings = embeddings\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            num_embeddings=num_unique_words,\n",
    "            embedding_dim=embedding_dim,\n",
    "            )\n",
    "\n",
    "        # RNN\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=num_hidden_units,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_p,\n",
    "            )\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights.\n",
    "        \"\"\"\n",
    "\n",
    "        # Freeze GloVe embeddings\n",
    "        self.word_embeddings.weight.data = self.embeddings\n",
    "        self.word_embeddings.weight.requires_grad=False\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden\n",
    "        states for RNN.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        states = Variable(\n",
    "            torch.zeros(\n",
    "                self.num_layers, self.batch_size, self.num_hidden_units))\n",
    "\n",
    "    def forward(self, inputs, indexes, states):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "\n",
    "        # Embed the inputs\n",
    "        embedded_inputs = self.word_embeddings(inputs)\n",
    "\n",
    "        # Feed into RNN\n",
    "        rnn_outputs, last_state = self.gru(embedded_inputs, states)\n",
    "\n",
    "        # Extract relevant state\n",
    "        last_relevant_state = torch.gather(\n",
    "            rnn_outputs,\n",
    "            dim=1,\n",
    "            index=indexes,\n",
    "            )\n",
    "\n",
    "        return last_relevant_state.view(self.batch_size, self.num_hidden_units)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_p, num_hidden_units, num_classes):\n",
    "        \"\"\"\n",
    "        Initialize weights.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_hidden_units, num_hidden_units)\n",
    "        self.dropout1 = nn.Dropout(dropout_p)\n",
    "        self.fc2 = nn.Linear(num_hidden_units, num_hidden_units)\n",
    "        self.dropout2 = nn.Dropout(dropout_p)\n",
    "        self.out = nn.Linear(num_hidden_units, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights.\n",
    "        \"\"\"\n",
    "        init.xavier_uniform(\n",
    "            self.fc1.weight,\n",
    "            gain=calculate_gain('relu'),\n",
    "            )\n",
    "        init.xavier_uniform(\n",
    "            self.fc2.weight,\n",
    "            gain=calculate_gain('relu'),\n",
    "            )\n",
    "\n",
    "    def forward(self, encoder_output):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        z = F.relu(self.fc1(encoder_output))\n",
    "        z = self.dropout1(z)\n",
    "        z = F.relu(self.fc2(z))\n",
    "        z = self.dropout2(z)\n",
    "        output = self.out(z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Future posts will cover specific example use cases with RNNs including attention mechanisms, tree based RNN structures, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
