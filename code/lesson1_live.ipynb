{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lesson 1: Introduction to Deep Learning with PyTorch\n",
    "- [@AlfredoCanziani](https://twitter.com/alfredocanziani)\n",
    "- [@GokuMohandas](https://twitter.com/GokuMohandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is PyTorch ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PyTorch is a python library for scientific computing that offers flexibility and speed. It's two main offerings are a high level **tensor library** with strong GPU acceleration and a **automatic differentiation** system for computational graphs.\n",
    "\n",
    "Reasons to use:\n",
    "* **Deep python integration** with minimal framework overhead and links with external libraries like NVIDIA (CuDNN, etc.).\n",
    "     \n",
    "* **Imperative style** makes it very flexible and transparent. You see exactly what is going to be run.\n",
    "        \n",
    "* **Dynamic computation graphs** makes a a graph for each instance (or minibatch) and is efficient for tasks with variable amounts of work per sample.\n",
    "        \n",
    "<img src=\"http://pytorch.org/static/img/dynamic_graph.gif\">\n",
    "<p1><center>Source: http://pytorch.org/static/img/dynamic_graph.gif</center></p1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "from pycrayon import (\n",
    "    CrayonClient,\n",
    ")\n",
    "\n",
    "from IPython import (\n",
    "    display,\n",
    ")\n",
    "\n",
    "from IPython.display import (\n",
    "    Image,\n",
    "    clear_output,\n",
    ")\n",
    "\n",
    "# get matplotlib configuration\n",
    "%matplotlib inline\n",
    "%run plot_conf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import (\n",
    "    Variable,\n",
    ")\n",
    "from torch.nn import (\n",
    "    init,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PyTorch basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pytorch.org/docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensor of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensor addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# running on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://pytorch.org/tutorials/_images/Variable.png\">\n",
    "<p1><center>Source: http://pytorch.org/tutorials/_images/Variable.png</center></p1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`autograd`** package is a key package for working with our computational graphs. It provides automatic differentiation for operations on tensors. **`Variable`** is the core class in the package and it wraps around our tensors. Once we defined our computations in the graph, we can call **`.backward()`** on our objective to calculate all the gradients automatically! We can always access the values in raw tensors with **`.data`** and the gradient w.r.t to our objective with **`.grad`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/computation_graph.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# + operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# * and mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p1><center>Forward Pass:</center></p1>\n",
    "$$ y = x + 4 $$\n",
    "$$ z = 3y $$\n",
    "$$ o = \\frac{1}{N}\\sum_{i=1}^{N}z_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p1><center>Backward Pass for <i>x</i>:</center></p1>\n",
    "$$ \\frac{\\partial{o}}{\\partial{x}} = \\frac{\\partial{o}}{\\partial{z}} \\frac{\\partial{z}}{\\partial{y}} \\frac{\\partial{y}}{\\partial{x}} = \\frac{1}{N} (3) (1) = \\frac{1}{9}(3) = \\frac{1}{3} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradients w.r.t o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see the data and creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p1><center>Creating differentiable computation graphs for classification tasks.</center></p1>\n",
    "<img src=\"images/train.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=12345\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "N = 1000  # num_samples_per_class\n",
    "D = 2  # dimensions\n",
    "C = 3  # num_classes\n",
    "H = 100  # num_hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.zeros(N * C, D)\n",
    "y = torch.zeros(N * C)\n",
    "\n",
    "for i in range(C):\n",
    "    index = 0\n",
    "    r = torch.linspace(0, 1, N)\n",
    "    t = torch.linspace(\n",
    "        i * 2 * math.pi / C,\n",
    "        (i + 2) * 2 * math.pi / C,\n",
    "        N\n",
    "    ) + torch.randn(N) * 0.1\n",
    "    \n",
    "    for ix in range(N * i, N * (i + 1)):\n",
    "        X[ix] = r[index] * torch.FloatTensor((\n",
    "            math.sin(t[index]), math.cos(t[index])\n",
    "        ))\n",
    "        y[ix] = i\n",
    "        index += 1\n",
    "\n",
    "print(\"SHAPES:\")\n",
    "print(\"-------------------\")\n",
    "print(\"X:\", tuple(X.size()))\n",
    "print(\"y:\", tuple(y.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_data(X, y, d=.0, auto=False):\n",
    "    \"\"\"\n",
    "    Plot the data.\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=plt.cm.Spectral)\n",
    "    plt.axis('square')\n",
    "    plt.axis((-1.1, 1.1, -1.1, 1.1))\n",
    "    if auto is True: plt.axis('equal')\n",
    "#     plt.savefig('spiral{:.2f}.png'.format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the data\n",
    "plot_data(X.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_model(X, y, model, e=.0, auto=False):\n",
    "    \"\"\"\n",
    "    Plot the model from torch weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X.numpy()\n",
    "    y = y.numpy(),\n",
    "    w1 = torch.transpose(model.fc1.weight.data, 0, 1).numpy()\n",
    "    b1 = model.fc1.bias.data.numpy()\n",
    "    w2 = torch.transpose(model.fc2.weight.data, 0, 1).numpy()\n",
    "    b2 = model.fc2.bias.data.numpy()\n",
    "    \n",
    "    h = 0.01\n",
    "\n",
    "    x_min, x_max = (-1.1, 1.1)\n",
    "    y_min, y_max = (-1.1, 1.1)\n",
    "    \n",
    "    if auto is True:\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = np.dot(np.maximum(0, np.dot(np.c_[xx.ravel(), yy.ravel()], w1) + b1), w2) + b2\n",
    "    Z = np.argmax(Z, axis=1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig = plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "    plt.axis((-1.1, 1.1, -1.1, 1.1))\n",
    "    plt.axis('square')\n",
    "    if auto is True:\n",
    "        plt.axis((xx.min(), xx.max(), yy.min(), yy.max()))\n",
    "    \n",
    "#     plt.savefig('train{:03.2f}.png'.format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "lambda_l2 = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear model\n",
    "class linear_model(nn.Module):\n",
    "    \"\"\"\n",
    "    Linaer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        Initialize weights.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nn package to create our linear model\n",
    "# each Linear module has a weight and bias\n",
    "\n",
    "\n",
    "# nn package also has different loss functions.\n",
    "# we use cross entropy loss for our classification task\n",
    "\n",
    "\n",
    "# we use the optim package to apply\n",
    "# stochastic gradient descent for our parameter updates\n",
    "\n",
    "\n",
    "# We convert our inputs and targets to Variables\n",
    "# so we can use automatic differentiation but we \n",
    "# use require_grad=False b/c we don't want the gradients\n",
    "# to alter these values.\n",
    "\n",
    "\n",
    "# Training\n",
    "    \n",
    "    # Feed forward to get the logits\n",
    "    \n",
    "    # Compute the loss and accuracy\n",
    "    \n",
    "    # zero the gradients before running\n",
    "    # the backward pass.\n",
    "    \n",
    "    # Backward pass to compute the gradient\n",
    "    # of loss w.r.t our learnable params. \n",
    "    \n",
    "    # Update params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-layered network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "lambda_l2 = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN model\n",
    "class two_layer_network(nn.Module):\n",
    "    \"\"\"\n",
    "    NN model.\n",
    "    \"\"\"\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        Initialize weights.\n",
    "        \"\"\"\n",
    "        super(two_layer_network, self).__init__()\n",
    "        self.fc1 = nn.Linear(D_in, H)\n",
    "        self.fc2 = nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nn package to create our linear model\n",
    "# each Linear module has a weight and bias\n",
    "model = two_layer_network(D, H, C)\n",
    "\n",
    "# nn package also has different loss functions.\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# we use the optim package to apply\n",
    "# ADAM for our parameter updates\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# We convert our inputs and targest to Variables\n",
    "# so we can use automatic differentiation but we \n",
    "# use require_grad=False b/c we don't want the gradients\n",
    "# to alter these values.\n",
    "input_X = Variable(X, requires_grad=False)\n",
    "y_true = Variable(y.long(), requires_grad=False)\n",
    "\n",
    "# e = 1.  # plotting purpose\n",
    "\n",
    "# Training\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Feed forward to get the logits\n",
    "    y_pred = model(input_X)\n",
    "    \n",
    "    # Compute the loss and accuracy\n",
    "    loss = criterion(y_pred, y_true)\n",
    "    score, predicted = torch.max(y_pred, 1)\n",
    "    acc = (y_true.data == predicted.data).sum() / float(len(y_true))\n",
    "    print(\"[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.data[0], acc))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # zero the gradients before running\n",
    "    # the backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient\n",
    "    # of loss w.r.t our learnable params. \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step()\n",
    "    \n",
    "#    # Plot some progress\n",
    "#     if t % math.ceil(e) == 0:\n",
    "#         plot_model(X, y, model, e)\n",
    "#         e *= 1.5\n",
    "\n",
    "#! convert -delay 20 -crop 500x475+330+50 +repage $(gls -1v train*) train.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "print(model)\n",
    "plot_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_ratio = 0.8 # train-test split\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "log_every = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle and split the data\n",
    "shuffle_indicies = torch.LongTensor(random.sample(range(0, len(X)), len(X)))\n",
    "X = X[shuffle_indicies]\n",
    "y = y[shuffle_indicies]\n",
    "test_start_idx = int(len(X) * split_ratio)\n",
    "X_train = X[:test_start_idx] \n",
    "y_train = y[:test_start_idx] \n",
    "X_test = X[test_start_idx:] \n",
    "y_test = y[test_start_idx:]\n",
    "print(\"We have %i train samples and %i test samples.\" % (len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join X and y\n",
    "train_data = torch.cat([X_train, y_train], 1)\n",
    "test_data = torch.cat([X_test, y_test], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "lambda_l2 = 1e-5\n",
    "dropout_p = 0.1\n",
    "decay_rate = 0.9999\n",
    "max_grad_norm = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom model\n",
    "class customized_network(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom model.\n",
    "    \"\"\"\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        Initialize weights.\n",
    "        \"\"\"\n",
    "        super(customized_network, self).__init__()\n",
    "        self.fc1 = nn.Linear(D_in, H)\n",
    "        self.fc2 = nn.Linear(H, D_out)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Properly initialize weights.\n",
    "        \"\"\"\n",
    "        init.xavier_uniform(self.fc1.weight, gain=np.sqrt(2.0)) # gain for ReLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        z = F.relu(self.fc1(x))\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc2(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_batch(batch, model, criterion, optimizer, is_training):\n",
    "    \"\"\"\n",
    "    Process a minibatch for loss and accuracy.\n",
    "    \"\"\"\n",
    "    X_batch = batch[:,:2]\n",
    "    y_batch = batch[:, 2]\n",
    "\n",
    "    # Convert tensors to Variables (for autograd)\n",
    "    X_batch = Variable(X_batch, requires_grad=False)\n",
    "    y_batch = Variable(y_batch.long(), requires_grad=False)\n",
    "\n",
    "    # Forward pass\n",
    "    scores = model(X_batch) # logits\n",
    "\n",
    "    # Loss\n",
    "    loss = criterion(scores, y_batch)\n",
    "    \n",
    "    # Accuracy\n",
    "    score, predicted = torch.max(scores, 1)\n",
    "    accuracy = (y_batch.data == predicted.data).sum() / float(len(y_batch))\n",
    "    \n",
    "    if is_training:\n",
    "\n",
    "        # Use autograd to do backprop. This will compute the\n",
    "        # gradients w.r.t loss for all Variables that have\n",
    "        # requires_grad=True. So, our w1 and w2 will now have\n",
    "        # gradient components we can access.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradient norms\n",
    "        nn.utils.clip_grad_norm(model.parameters(), max_grad_norm)\n",
    "\n",
    "        # Update params\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics(train_loss, train_acc, test_loss, test_acc):\n",
    "    \"\"\"\n",
    "    Plot the global metrics.\n",
    "    \"\"\"\n",
    "    f, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\n",
    "    \n",
    "    ax1.plot(train_loss, label='train loss')\n",
    "    ax1.plot(test_loss, label='test loss')\n",
    "    ax1.legend(loc=3)\n",
    "    ax1.set_title('Loss')\n",
    "    \n",
    "    ax2.plot(train_acc, label='train accuracy')\n",
    "    ax2.plot(test_acc, label='test accuracy')\n",
    "    ax2.legend(loc=4)\n",
    "    ax2.set_title('Accuracy')\n",
    "    \n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proper_train(model, criterion, optimizer, train_loader, test_loader, \n",
    "                num_epochs, batch_size, log_every, learning_rate, lambda_l2, \n",
    "                dropout_p, decay_rate, max_grad_norm, animate=False):\n",
    "    \"\"\"\n",
    "    Training with a few extra (nice) components.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Metrics\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "\n",
    "    # Training\n",
    "    for num_train_epoch in range(num_epochs):\n",
    "\n",
    "        # Timer\n",
    "        start = time.time()\n",
    "\n",
    "        # Decay learning rate\n",
    "        learning_rate = learning_rate * (decay_rate ** (num_train_epoch // 1.0))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "        # Metrics\n",
    "        train_batch_loss = 0.0\n",
    "        train_batch_accuracy = 0.0\n",
    "\n",
    "        for train_batch_num, (train_batch) in enumerate(train_loader):\n",
    "\n",
    "            # Get metrics\n",
    "            model.train()\n",
    "            loss, accuracy = process_batch(train_batch, model, criterion, optimizer, model.training)\n",
    "\n",
    "            # Add to batch scalars\n",
    "            train_batch_loss += loss.data[0] / float(len(train_batch))\n",
    "            train_batch_accuracy += accuracy\n",
    "            \n",
    "        # Add to global metrics\n",
    "        train_loss.append(train_batch_loss/float(train_batch_num+1))\n",
    "        train_acc.append(train_batch_accuracy/float(train_batch_num+1))\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        for num_test_epoch in range(1):\n",
    "\n",
    "            # Metrics\n",
    "            test_batch_loss = 0.0\n",
    "            test_batch_accuracy = 0.0\n",
    "\n",
    "            for test_batch_num, (test_batch) in enumerate(test_loader):\n",
    "\n",
    "                # Get metrics\n",
    "                model.eval()\n",
    "                loss, accuracy = \\\n",
    "                    process_batch(test_batch, model, criterion, optimizer, model.training)\n",
    "\n",
    "                # Add to batch scalars\n",
    "                test_batch_loss += loss.data[0] / float(len(test_batch))\n",
    "                test_batch_accuracy += accuracy\n",
    "\n",
    "            # Add to global metrics\n",
    "            test_loss.append(test_batch_loss/float(test_batch_num+1))\n",
    "            test_acc.append(test_batch_accuracy/float(test_batch_num+1))\n",
    "                \n",
    "            if animate:\n",
    "                verbose_condition = num_train_epoch == num_epochs-1\n",
    "                if num_train_epoch%10 == 0:\n",
    "                    plot_model(X, y, model)\n",
    "                    display.clear_output(wait=True)\n",
    "            else:\n",
    "                verbose_condition = \\\n",
    "                    (num_train_epoch == 0) or (num_train_epoch % log_every == 0) or (num_train_epoch == num_epochs-1)\n",
    "\n",
    "            # Verbose\n",
    "            if verbose_condition:\n",
    "\n",
    "                # Verbose\n",
    "                time_remain = (time.time() - start) * (num_epochs - (num_train_epoch+1))\n",
    "                minutes = time_remain // 60\n",
    "                seconds = time_remain - minutes*60\n",
    "                print(\"TIME REMAINING: %im %is\" % (minutes, seconds))\n",
    "                print(\"[EPOCH]: %i, [TRAIN LOSS]: %.6f, [TRAIN ACC]: %.3f, [TEST LOSS]: %.6f, [TEST ACC]: %.3f\" %\n",
    "                       (num_train_epoch, train_batch_loss/float(train_batch_num+1), \n",
    "                        train_batch_accuracy/float(train_batch_num+1), test_batch_loss/float(test_batch_num+1),\n",
    "                        test_batch_accuracy/float(test_batch_num+1)))\n",
    "            \n",
    "    # Plot global metrics\n",
    "    plot_metrics(train_loss, train_acc, test_loss, test_acc)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = customized_network(D_in=D, H=H, D_out=C)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# Train\n",
    "model = proper_train(model, criterion, optimizer, train_loader, test_loader, \n",
    "                num_epochs, batch_size, log_every, learning_rate, lambda_l2, \n",
    "                dropout_p, decay_rate, max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "print(model)\n",
    "plot_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_infer = Variable(X[0].view(1, -1))\n",
    "y_infer = Variable(y[:1])\n",
    "print(\"X_infer:\\n\", X_infer)\n",
    "print(\"y_infer:\\n\", y_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feed through model to get probs\n",
    "\n",
    "# Sorted probabilities\n",
    "sorted_, indices = torch.sort(probabilities, descending=True)\n",
    "for i, index in enumerate(indices[0]):\n",
    "    print(\"%i - %i%%\" % (\n",
    "        indices[0].data[i],\n",
    "        100.0*probabilities.data[0][index.data[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how changing our hyperparameters can alter the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = customized_network(D_in=D, H=H, D_out=C)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "lambda_l2 = 1e-5\n",
    "dropout_p = 0.1\n",
    "decay_rate = 0.9999\n",
    "max_grad_norm = 5.0\n",
    "\n",
    "model = proper_train(model, criterion, optimizer, train_loader, test_loader, \n",
    "                num_epochs, batch_size, log_every, learning_rate, lambda_l2, \n",
    "                dropout_p, decay_rate, max_grad_norm, animate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "print(model)\n",
    "plot_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization / debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Takes more time to run models but is very useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to the server for Crayon (tensorboard)\n",
    "cc = CrayonClient(hostname=\"localhost\", port=8889)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new experiment\n",
    "exp_name = \"custom_model\"\n",
    "try:\n",
    "    cc.remove_experiment(exp_name)\n",
    "    exp = cc.create_experiment(exp_name)\n",
    "except:\n",
    "    exp = cc.create_experiment(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crayon_train(exp, model, criterion, optimizer, train_loader, test_loader, \n",
    "                num_epochs, batch_size, log_every, learning_rate, lambda_l2, \n",
    "                dropout_p, decay_rate, max_grad_norm, animate=False):\n",
    "    \"\"\"\n",
    "    Training with a few extra (nice) components.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training\n",
    "    for num_train_epoch in range(num_epochs):\n",
    "\n",
    "        # Timer\n",
    "        start = time.time()\n",
    "\n",
    "        # Decay learning rate\n",
    "        learning_rate = learning_rate * (decay_rate ** (num_train_epoch // 1.0))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "        # Metrics\n",
    "        train_batch_loss = 0.0\n",
    "        train_batch_accuracy = 0.0\n",
    "\n",
    "        for train_batch_num, (train_batch) in enumerate(train_loader):\n",
    "\n",
    "            # Get metrics\n",
    "            model.train()\n",
    "            loss, accuracy = process_batch(train_batch, model, criterion, optimizer, model.training)\n",
    "\n",
    "            # Collect weights\n",
    "            exp.add_histogram_value(\n",
    "                \"W2\", hist=list(model.fc2.weight.data.view(-1, )), tobuild=True)\n",
    "            \n",
    "            # Add to batch scalars\n",
    "            train_batch_loss += loss.data[0] / float(len(train_batch))\n",
    "            train_batch_accuracy += accuracy\n",
    "            \n",
    "            # Record metrics\n",
    "            exp.add_scalar_value(\"train_loss\", value=loss.data[0] / float(len(train_batch)))\n",
    "            exp.add_scalar_value(\"train_accuracy\", value=accuracy)\n",
    "            \n",
    "        if animate:\n",
    "            verbose_condition = num_train_epoch == num_epochs-1\n",
    "            if num_train_epoch%10 == 0:\n",
    "                plot_model(X, y, model)\n",
    "                display.clear_output(wait=True)\n",
    "        else:\n",
    "            verbose_condition = \\\n",
    "                (num_train_epoch == 0) or (num_train_epoch % log_every == 0) or (num_train_epoch == num_epochs-1)\n",
    "\n",
    "        # Verbose\n",
    "        if verbose_condition:\n",
    "\n",
    "            # Testing\n",
    "            model.eval()\n",
    "            for num_test_epoch in range(1):\n",
    "\n",
    "                # Metrics\n",
    "                test_batch_loss = 0.0\n",
    "                test_batch_accuracy = 0.0\n",
    "\n",
    "                for test_batch_num, (test_batch) in enumerate(test_loader):\n",
    "\n",
    "                    # Get metrics\n",
    "                    model.eval()\n",
    "                    loss, accuracy = \\\n",
    "                        process_batch(test_batch, model, criterion, optimizer, model.training)\n",
    "\n",
    "                    # Add to batch scalars\n",
    "                    test_batch_loss += loss.data[0] / float(len(test_batch))\n",
    "                    test_batch_accuracy += accuracy\n",
    "                    \n",
    "                    # Record metrics\n",
    "                    exp.add_scalar_value(\"test_loss\", value=loss.data[0] / float(len(test_batch)))\n",
    "                    exp.add_scalar_value(\"test_accuracy\", value=accuracy)\n",
    "\n",
    "            # Verbose\n",
    "            time_remain = (time.time() - start) * (num_epochs - (num_train_epoch+1))\n",
    "            minutes = time_remain // 60\n",
    "            seconds = time_remain - minutes*60\n",
    "            print(\"TIME REMAINING: %im %is\" % (minutes, seconds))\n",
    "            print(\"[EPOCH]: %i, [TRAIN LOSS]: %.6f, [TRAIN ACC]: %.3f, [TEST LOSS]: %.6f, [TEST ACC]: %.3f\" %\n",
    "                   (num_train_epoch, train_batch_loss/float(train_batch_num+1), \n",
    "                    train_batch_accuracy/float(train_batch_num+1), test_batch_loss/float(test_batch_num+1),\n",
    "                    test_batch_accuracy/float(test_batch_num+1)))\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crayon_train(exp, model, criterion, optimizer, train_loader, test_loader, \n",
    "                num_epochs, batch_size, log_every, learning_rate, lambda_l2, \n",
    "                dropout_p, decay_rate, max_grad_norm, animate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/scalars-k.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/weights-k.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Caution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "More data we have, the larger we can make our models and not suffer from overfitting. Things like dropout and finetuning regularization etc. will help us with getting better generalization but we still need to very cautious. In fact, with enough parameters, we can overfit to **completely random data**...\n",
    "\n",
    "```\n",
    "FINITE-SAMPLE EXPRESSIVITY:\n",
    "\n",
    "\"There exists a two-layer neural network with ReLU activations and 2n+d weights that can represent any function on a sample of size n in d dimensions”. This really demonstrates the brute-force capacity of neural nets for any dataset. The proof can be found in the appendix of https://arxiv.org/abs/1611.03530.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "C = 4\n",
    "H = 2*N*C + D\n",
    "split_ratio = 0.95\n",
    "X = torch.randn(N*C, D)\n",
    "y = torch.FloatTensor([[i]*N for i in range(C)]).view(-1)\n",
    "\n",
    "print(\"SHAPES:\")\n",
    "print(\"-------------------\")\n",
    "print(\"X:\", X.size())\n",
    "print(\"y:\", y.size())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the data\n",
    "plot_data(X.numpy(), y.numpy(), auto=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle_indicies = torch.LongTensor(random.sample(range(0, len(X)), len(X)))\n",
    "X = X[shuffle_indicies]\n",
    "y = y[shuffle_indicies]\n",
    "test_start_idx = int(len(X) * split_ratio)\n",
    "X_train = X[:test_start_idx] \n",
    "y_train = y[:test_start_idx] \n",
    "X_test = X[test_start_idx:] \n",
    "y_test = y[test_start_idx:]\n",
    "print(\"We have %i train samples and %i test samples.\" % (len(X_train), len(X_test)))\n",
    "\n",
    "# Join X and y\n",
    "train_data = torch.cat([X_train, y_train], 1)\n",
    "test_data = torch.cat([X_test, y_test], 1)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = customized_network(D_in=D, H=H, D_out=C)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "num_epochs = 20000\n",
    "log_every = 5000\n",
    "learning_rate = 1e-3\n",
    "lambda_l2 = 0\n",
    "dropout_p = 0.0\n",
    "decay_rate = 1\n",
    "max_grad_norm = 50.0\n",
    "\n",
    "model = proper_train(model, criterion, optimizer, train_loader, test_loader, \n",
    "                num_epochs, batch_size, log_every, learning_rate, lambda_l2, \n",
    "                dropout_p, decay_rate, max_grad_norm, animate=False)\n",
    "# Plot trained model\n",
    "print(model)\n",
    "plot_model(X, y, model, auto=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
